Project Lumina: A Roadmap for an Interactive AI Astrologer & Her Evolving Digital World

Document Version: 1.0
Date: May 6, 2025
Section 1: Introduction & Vision
1.1. Overview of Lumina

Lumina is envisioned as a wise, empathetic, and engaging AI Astrologer. She will manifest as a 2D cartoon character, accompanied by her loyal dog, interacting with audiences in real-time. Her primary role is to offer astrological insights, tarot readings, and mystical guidance, fostering a sense of connection and wonder.
1.2. The Grand Vision: A Self-Generating World

Beyond the initial MVP, Lumina is conceived as the central figure in a vast, self-generating digital world. This world will:

    Be an "Infinite Canvas": A sprawling 2D environment that expands over time, introducing new locations, characters, and interactive elements.

    Host Self-Replicating AI Agent Influencers: Other AI personas, potentially generated procedurally, will inhabit this world. They will have their own personalities, goals, and social media presences, interacting with Lumina, each other, and the audience.

    Develop Emergent Storylines: Interactions between these AI agents, including "beefs" and collaborations, will create dynamic, unscripted narratives.

    Continuously Evolve: Fueled by real-world data (news, trends) and Lumina's (and other agents') interactions, the world and its stories will constantly be fresh and developing.

1.3. Core Philosophy: Interactivity & Novelty > Animation Polish

The driving principle behind Lumina is that deep, novel interactivity and intelligent, evolving content are more compelling than hyper-polished but static animations. The focus is on creating a unique experience that captivates through genuine engagement and a sense of a living digital presence.
Section 2: Minimum Viable Product (MVP) - Lumina Live & Socially Active

The MVP will focus on bringing Lumina and her dog to life as a live streamer and establishing her initial social media presence.
2.1. Core MVP Features:

    Lumina & Dog (Visual Representation):

        Simple, appealing 2D cartoon characters.

        Rendered using HTML, CSS, and JavaScript in a web browser.

        Assets: Layered PNGs for body, head, mouth shapes, eye states, dog.

    Basic Animations:

        Mouth Movement: Synchronized with Lumina's TTS (Text-to-Speech) output. This will likely be a simple swap between 2-3 mouth PNGs (closed, slightly open, open).

        Blinking: Periodic, natural-looking blinks for Lumina and her dog.

        "Jazz" Animations: Simple, untimed animations to add life, e.g., subtle idle body sway, tail wag for the dog, occasional sparkles or visual cues. These are not tied to specific actions but loop or trigger randomly.

    AI-Powered Interaction:

        Brain: A Large Language Model (LLM) running locally via LM Studio, configured with Lumina's persona.

        Interaction API: A Python-based backend will expose an API for sending user chat/prompts to Lumina (LM Studio) and receiving her responses.

    Real-time Voice (TTS):

        Lumina's text responses converted to speech using a fast, pre-trained TTS solution.

    Live Streaming to YouTube:

        Headless capture of the browser window rendering Lumina.

        Audio from TTS mixed in.

        Streamed to YouTube Live via RTMP, managed by Python scripts.

    Basic Social Media Posting:

        Automated posting of simple content (e.g., "Cosmic Thought of the Day," "Card of the Day") to at least one platform (e.g., X/Twitter).

        Content generated by Lumina's LLM.

2.2. Technology Stack for MVP:

    Frontend (Lumina's Visuals & Animation):

        HTML5: Structure for character elements.

        CSS3: Styling and positioning of character layers.

        JavaScript (ES6+): DOM manipulation, animation logic, communication with backend.

            Consider GSAP (GreenSock Animation Platform): For more sophisticated "jazz" animations and potentially smoother transitions, though simple JS can suffice for MVP.

    Backend (Orchestration, API, Social Media, Streaming Control):

        Python 3.9+

        Web Framework (Optional but Recommended for API): FastAPI or Flask for creating the interaction API.

        HTTP Client: requests or aiohttp (for async operations, beneficial for real-time).

    AI Brain:

        LM Studio (latest version).

        A chosen GGUF-compatible LLM (e.g., a Mixtral, Llama 3, or other capable model that fits your hardware and performs well with instruction following and persona).

    Text-to-Speech (TTS):

        Recommendation: Piper (local, fast, good quality pre-trained voices, open-source) or Coqui TTS (more flexible, but potentially more setup).

        Cloud Alternative (if budget allows and for ease/quality): ElevenLabs, Google Cloud TTS, Azure Cognitive Services TTS. For MVP, a local solution might be preferable to manage costs and reduce latency if your hardware can handle it alongside the LLM.

    Live Streaming Infrastructure:

        Headless Browser Environment (on your server/streaming PC):

            Playwright (for Python browser automation).

            Xvfb (X virtual framebuffer) on Linux to run the browser headlessly.

        Encoding & Streaming: FFmpeg (command-line tool, controlled via Python subprocess).

        YouTube Integration: Google API Python Client (google-api-python-client) for YouTube Data API v3 (Live Streaming).

    Social Media Integration:

        Python libraries specific to each platform (e.g., tweepy for X, instagrapi for Instagram - be mindful of API terms of service).

Section 3: Roadmap to MVP Deployment ("Fastest Deployment")

This roadmap prioritizes getting Lumina live and posting with core features.
Phase 1: Core Setup & Character Foundation (Est. 1-2 Weeks)

    Environment Setup:

        Install Python, Node.js (if needed for any frontend tools).

        Set up a virtual environment for your Python project.

        Install FFmpeg and ensure it's in your system PATH.

    LM Studio Installation & Configuration (Detailed in Section 5.1):

        Download and install LM Studio.

        Select and download a suitable LLM (e.g., a 7B or 13B parameter model that's good for chat and instruction following, quantized for your 3090 Ti).

        Test basic chat interaction with the model within LM Studio.

        Configure and start the LM Studio local API server. Note the port and model identifier.

    Lumina's Persona Prompt v1:

        Draft the initial detailed system prompt for Lumina (as discussed previously, focusing on her astrologer persona, speaking style, basic knowledge areas for MVP).

        Test this prompt directly in LM Studio's chat interface.

    Basic 2D Assets:

        Create/finalize the layered PNGs for Lumina:

            Base body/head.

            Mouth (closed, slightly open, open for talking).

            Eyes (open, closed for blinking).

        Create/finalize assets for Lumina's dog (e.g., base body, blinking eyes, simple tail wag frames).

        Organize these assets into a clear folder structure (see Section 5.3).

    HTML/CSS Structure for Lumina's Scene:

        Create an HTML file (lumina_stage.html).

        Use div elements for Lumina, her dog, and any background elements.

        Use CSS to layer the PNGs correctly (e.g., using position: absolute, z-index).

        Ensure the initial state (e.g., mouth closed, eyes open) is displayed correctly.

        Self-Correction: Initially, I might just put all CSS in the HTML. For better organization, even for MVP, a separate style.css is better.

Phase 2: Basic Interaction & Animation (Est. 2-3 Weeks)

    Python Backend - API & LM Studio Integration:

        Set up a simple Python web server (FastAPI/Flask) or a script to listen for inputs.

        Write Python code to:

            Receive a text prompt (e.g., from a test interface or later from chat).

            Construct the JSON payload for LM Studio's API (/v1/chat/completions), including Lumina's system prompt and the user's message.

            Send the request to the LM Studio API and get the text response.

    TTS Integration:

        Choose your TTS solution (e.g., Piper).

        Install and configure it. If local, ensure you have the pre-trained voice model files.

        Write Python code to take Lumina's text response from LM Studio and send it to the TTS engine to generate audio (e.g., save to a temporary .wav file or stream bytes).

    JavaScript for Basic Animations:

        In your lumina_stage.html (or a linked animation.js file):

            Mouth Sync Logic:

                When TTS audio starts playing (your Python backend will need to signal this or JS detects audio playback), cycle through mouth PNGs. A simple approach: open mouth when audio is active, closed when silent. More advanced: analyze audio volume or use a library if available for very basic viseme-like changes.

                Self-Correction: Directly detecting "audio playing" from a Python-triggered TTS in browser JS can be tricky. A simpler MVP is for Python to send a "start_talking" and "stop_talking" cue to the JS via a WebSocket or simple polling, and JS changes mouth state accordingly.

            Blinking Logic: Implement a JavaScript setInterval to randomly trigger eye-closed/eye-open states for Lumina and her dog.

            "Jazz" Animations: Add simple CSS animations or JavaScript-driven loops for subtle movements (e.g., dog's tail wagging, Lumina's slight sway).

    Connecting Backend to Frontend (MVP Level):

        For MVP, the Python script generating TTS audio could also trigger basic animation cues. A simple way is to have the Python script update a small JSON file or send a simple HTTP request to a local endpoint that the JavaScript on lumina_stage.html polls frequently for animation state changes (e.g., {"is_talking": true, "blink_now": false}).

        Better for responsiveness (if time allows for MVP): Use WebSockets for real-time communication between the Python backend and the JavaScript frontend. Python sends messages (LLM response for TTS, animation cues), JS receives and acts.

Phase 3: Live Streaming Setup (Est. 2-3 Weeks)

    Headless Browser & Virtual Display:

        Install Playwright: pip install playwright then playwright install.

        On Linux, install Xvfb: sudo apt-get install xvfb.

        Write a Python script to:

            Start Xvfb on a specific display number (e.g., :99).

            Launch a Playwright-controlled browser (e.g., Chromium) within that Xvfb display, opening lumina_stage.html.

            Ensure your HTML/JS animations are running correctly in this headless environment.

    FFmpeg Integration:

        Develop Python subprocess calls to FFmpeg.

        Video Source: Use x11grab to capture from the Xvfb display.

        Audio Source: This is key. The audio from your Python-driven TTS needs to be captured by FFmpeg.

            Option 1 (Simpler): If TTS outputs to a file, FFmpeg can read it. But for live, you need a stream.

            Option 2 (Better): Configure your system audio (e.g., PulseAudio on Linux) to create a virtual audio output/loopback device. Your Python TTS script plays audio to this virtual device, and FFmpeg captures from it. This requires OS-level audio configuration.

            Option 3 (Direct Pipe - Advanced): Pipe raw audio bytes from your TTS process directly to FFmpeg's stdin if your TTS library and FFmpeg setup allow.

        Encoding: H.264 for video, AAC for audio.

        Output: RTMP stream.

    YouTube Live Streaming API Integration:

        Set up a project in Google Cloud Console, enable YouTube Data API v3.

        Create OAuth 2.0 credentials. For a headless server, you'll likely need to:

            Perform an initial authorization flow to get a refresh token.

            Store this refresh token securely.

            Your Python script will use the refresh token to obtain access tokens programmatically.

        Write Python functions using google-api-python-client to:

            Create a liveStream resource (get RTMP URL and stream key).

            Create a liveBroadcast resource (title, description, privacy).

            Bind the stream to the broadcast.

            Transition the broadcast to testing, then live.

            (Later) Transition to complete.

    End-to-End Test:

        Run the full Python orchestration: starts Xvfb, browser, Lumina's scene, LM Studio interaction, TTS, FFmpeg streaming to YouTube, and YouTube API management.

        Verify the stream on YouTube. Debug audio/video sync, quality, and stability.

Phase 4: Basic Social Media Automation (Est. 1-2 Weeks)

    Choose Initial Platform & Setup API Access:

        Start with one platform (e.g., X/Twitter).

        Apply for developer API access and get your API keys/tokens.

    Python Script for Posting:

        Use the platform's Python library (e.g., tweepy).

        Write functions to:

            Authenticate with the API.

            Post a text update.

            (Optional for MVP) Post text with a static image of Lumina.

    LLM for Content Generation:

        Create prompts for Lumina to generate short, engaging social media content (e.g., "Lumina, compose a mystical one-sentence thought for the day for X.").

        Integrate this with your LM Studio API interaction script.

    Basic Scheduling/Triggering:

        For MVP, this could be a simple cron job (Linux) or Task Scheduler (Windows) that runs your Python posting script at set intervals.

        Or, the script could post once when Lumina "starts her day" (when you run the main application).

Section 4: Post-MVP - Building the "Giant Canvas" & Evolving World

This is where the grand vision starts to unfold, building upon the stable MVP.
4.1. Long-Term Memory & Knowledge for Lumina

    Vector Database Setup:

        Choose a vector database:

            Local/Self-Hosted: ChromaDB, Weaviate (can run in Docker), FAISS (library, not a full DB). Good for starting and control.

            Cloud-Managed: Pinecone, Zilliz Cloud. Easier to scale, but incurs costs.

        Install and configure your chosen database.

    Embedding Model:

        Choose a sentence transformer model (e.g., from Hugging Face's sentence-transformers library like all-MiniLM-L6-v2) to convert text into vector embeddings. This can run locally.

    Interaction Logging & Embedding Pipeline (Separate Process):

        Capture Interactions: Your main Python orchestration script for Lumina's live sessions logs all significant interactions (user input, Lumina's full response, any actions taken like tarot card "drawn").

        Offline/Batch Processing Script:

            Reads these logs.

            Generates embeddings for the text portions.

            Stores the original text, its embedding, timestamps, user identifiers (if any), and other relevant metadata into the vector database.

    Data Harvesting & Knowledge Ingestion Pipeline (Separate Process):

        Crawlers/API Clients: Develop Python scripts to:

            Scrape relevant websites (astrology blogs, esoteric news, general news for topical context).

            Connect to news APIs.

            (Future) Monitor specific social media trends or topics.

        Processing & Embedding:

            Clean and chunk the harvested text.

            Generate summaries (potentially using another LLM call).

            Create embeddings for these processed chunks/summaries.

            Store in the vector database, tagged with source, date, topic.

    Integrating Memory/Knowledge into Lumina's Prompts:

        When Lumina receives a prompt, your Python script will:

            Generate an embedding for the current query/context.

            Query the vector database for the N most similar past interactions or knowledge snippets.

            Format this retrieved information concisely.

            Prepend it to the next prompt sent to LM Studio, e.g., "[Relevant Past Info: ...]\n[Current News Snippet: ...]\nSystem Prompt for Lumina...\nUser: ..."

4.2. Expanding the World: The "Giant Canvas"

    HTML/CSS/JS for the Infinite Canvas:

        Large Container: A main div with dimensions far exceeding the viewport.

        Viewport: A fixed-size div with overflow: hidden;.

        Camera/Panning Logic (JavaScript):

            Control the transform: translate(x, y); or scrollLeft/scrollTop of the large container within the viewport to simulate camera movement.

            This can be triggered by Lumina "deciding" to go somewhere, user commands, or narrative events.

    Procedural Content Generation:

        Locations: Define templates or components for different types of locations (e.g., mystical forest, cosmic library, city street). JavaScript can dynamically assemble and add these to the "giant canvas" DOM.

        Characters (NFT-Style Mixing + LLM):

            As you described: have a library of character parts (heads, bodies, accessories).

            Python script (or JS) can randomly combine these to create a new visual character.

            Send a description of the new character (or its parts) to Lumina's LLM (or a dedicated "character creation" LLM prompt) to generate:

                Name.

                Brief backstory.

                Personality traits.

                Their "first monologue" or introductory lines.

            This new character (HTML/CSS/JS representation + AI persona) is then added to the canvas and the interaction logic.

        Interactive Elements: Add clickable objects, hidden clues, or mini-games to the canvas.

4.3. Advanced AI Interactions & Storylines

    Sophisticated Function Calling:

        Expand the range of "tools" Lumina (and other future AI agents) can request via function calling in LM Studio.

        Examples: get_weather_for_location(location), check_stock_price(ticker), summarize_web_page(url), generate_image_for_tarot_card(card_name).

    AI Agent Influencers:

        Each new AI character gets its own instance of an LLM persona (could be run via separate LM Studio instances if resources allow, or managed by a more complex central LLM router).

        They get their own "goals," knowledge snippets, and potentially their own social media posting scripts.

        They can be prompted to interact with Lumina, the audience, or each other based on the evolving storyline or random events.

    Orchestrating Interactions & "Beefs":

        A "Story Director" module in your Python backend can:

            Monitor interactions and world state.

            Introduce plot hooks or challenges.

            Prompt specific AI agents to interact in certain ways (e.g., "Agent_Bob, you disagree with Lumina's latest cosmic forecast. Express your skepticism on X and tag her.").

    Evolving Storylines:

        The combination of agent interactions, audience input, real-world data integration, and the Story Director's nudges will create emergent narratives.

        Key events and outcomes can be logged to the vector database, influencing future agent behavior and plot developments.

Section 5: Technical Deep Dives
5.1. LM Studio Setup (Detailed Steps)

    Download LM Studio:

        Go to https://lmstudio.ai/.

        Download the installer for your operating system (Windows, macOS, Linux). You have 3090 Tis, so likely Windows or Linux.

    Installation:

        Run the installer and follow the on-screen prompts.

    Choosing & Downloading Models:

        Launch LM Studio.

        Navigate to the "Search" tab (magnifying glass icon) on the left.

        Search for models. Good starting points for your hardware and needs (persona, chat, potential function calling) might be:

            Mixtral series (e.g., Mixtral-8x7B-Instruct-v0.1)

            Llama-3 series (e.g., Llama-3-8B-Instruct or Llama-3-70B-Instruct if your VRAM allows for higher quantizations of the larger model).

            Look for GGUF format models, as they are optimized for local inference.

            Pay attention to quantization levels (e.g., Q4_K_M, Q5_K_M). Higher quality quantizations use more VRAM but offer better performance. Start with a medium quantization for a 7B or 13B model.

        Click "Download" for the desired model file.

    Loading a Model & Chatting:

        Go to the "AI Chat" tab (speech bubble icon).

        At the top, select the model you downloaded from the dropdown menu.

        The model will load into memory (this can take some time and VRAM). Monitor the resource usage in LM Studio.

        Once loaded, you can chat with it directly in the interface to test its base capabilities and your persona prompts.

    Starting the Local API Server:

        Click on the "Local Server" tab (often looks like <-> or a server icon) on the left panel.

        Select Model: Choose the loaded model you want to serve from the dropdown at the top.

        Server Controls: You should see options to "Start Server."

        Configuration (Defaults are usually fine to start):

            Port: Default is usually 1234.

            Host: Default is 127.0.0.1 (localhost). You might need to change to 0.0.0.0 if you want to access it from another device on your local network (use with caution).

            OpenAI Compatible API: Ensure this is the mode you're using (it's typically the default or a clear option).

        Click "Start Server."

    Basic API Testing:

        Once the server is running, LM Studio will usually show you example curl commands or the API endpoint URL (e.g., http://localhost:1234/v1/chat/completions).

        You can test this with curl from your terminal:

        curl http://localhost:1234/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
          "model": "loaded-model-name-from-lm-studio", # Get this exact name from LM Studio
          "messages": [
            { "role": "system", "content": "You are a helpful assistant." },
            { "role": "user", "content": "Hello, who are you?" }
          ],
          "temperature": 0.7
        }'

        Or using a simple Python script with the requests library:

        import requests
        import json

        LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"
        # Find the exact model identifier from LM Studio after loading it
        # It might look like "local-model" or a more specific path/name
        MODEL_IDENTIFIER = "local-model" # Replace with actual identifier shown in LM Studio server tab

        headers = {"Content-Type": "application/json"}
        data = {
            "model": MODEL_IDENTIFIER,
            "messages": [
                {"role": "system", "content": "You are Lumina, a wise AI astrologer."},
                {"role": "user", "content": "Greetings Lumina! What is the energy of today?"}
            ],
            "temperature": 0.7,
            # "max_tokens": 500, # Optional
            # "stream": False # Set to True for streaming responses
        }

        try:
            response = requests.post(LM_STUDIO_URL, headers=headers, json=data)
            response.raise_for_status()  # Raise an exception for bad status codes
            completion = response.json()
            if completion.get('choices') and len(completion['choices']) > 0:
                print(completion['choices'][0]['message']['content'])
            else:
                print("No choices returned or unexpected response format.")
                print("Full response:", completion)
        except requests.exceptions.RequestException as e:
            print(f"API Request failed: {e}")
        except json.JSONDecodeError:
            print(f"Failed to decode JSON response: {response.text}")
        except KeyError:
            print(f"Unexpected response format from API: {completion}")


        Self-Correction: The MODEL_IDENTIFIER is crucial. LM Studio's server tab will show the exact identifier for the loaded model that the API expects. It's often just "local-model" or the filename of the GGUF file. Users must check this in their LM Studio UI.

5.2. Recommended TTS Solutions (Fast, Pre-trained)

For live streaming, TTS speed and naturalness are key.

    Piper (Local, Open Source):

        Pros: Very fast, good quality voices, runs entirely locally (good for privacy and no per-character costs), designed for low-resource devices but scales well. Many pre-trained voice models available in different languages and accents.

        Cons: Voice selection is limited to what's pre-trained and available. Custom voice cloning is more involved.

        Setup:

            Download Piper binaries and voice models from https://github.com/rhasspy/piper.

            You can call it via command line from Python:

            import subprocess
            import wave
            import io

            def text_to_speech_piper(text, voice_model_path, output_wav_file="output.wav"):
                # Ensure the voice model path is correct and accessible
                command = [
                    'piper',
                    '--model', voice_model_path,
                    '--output_file', output_wav_file
                ]
                # Piper expects text via stdin
                process = subprocess.Popen(command, stdin=subprocess.PIPE)
                process.communicate(input=text.encode('utf-8'))
                if process.returncode != 0:
                    print(f"Piper TTS failed with error code {process.returncode}")
                    return False
                print(f"TTS audio saved to {output_wav_file}")
                return True

            # Example usage:
            # LUMINA_VOICE_MODEL = "path/to/your/downloaded/en_US-ljspeech-high.onnx" # Replace with actual path
            # success = text_to_speech_piper("Hello, I am Lumina, your AI astrologer.", LUMINA_VOICE_MODEL)
            # if success:
            #     # Now you can play "output.wav" or stream it to FFmpeg
            #     pass

            For live streaming, you'd want to stream Piper's output bytes directly to FFmpeg if possible, or use a fast temporary file/named pipe.

    Coqui TTS (Local, Open Source):

        Pros: Highly flexible, many models available, supports voice cloning (though this takes effort). Actively developed.

        Cons: Can be more complex to set up and get optimal performance than Piper. Some models can be slower.

        Setup: pip install TTS. Then use their Python API.

        # from TTS.api import TTS
        # import sounddevice as sd # For playback, or save to file for FFmpeg
        # import numpy as np

        # # Example: (Ensure you have a model downloaded or specify one)
        # # List available models: TTS.list_models()
        # # tts_model_name = "tts_models/en/ljspeech/tacotron2-DDC" # Example model
        # try:
        #     print("Initializing Coqui TTS...")
        #     # Get device
        #     device = "cuda" if torch.cuda.is_available() else "cpu" # Example, Coqui might handle this internally
        #     tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=True).to(device) # Replace with your chosen model
        #     print("Coqui TTS Initialized.")

        #     text = "Hello, I am Lumina, your AI astrologer."
        #     print(f"Synthesizing: {text}")
        #     # Synthesize to a file or directly to audio stream
        #     wav_output = tts.tts(text=text, speaker=tts.speakers[0] if tts.speakers else None, language=tts.languages[0] if tts.languages else None)

        #     # To save to file for FFmpeg:
        #     # tts.tts_to_file(text=text, speaker=tts.speakers[0], language=tts.languages[0], file_path="output_coqui.wav")

        #     # For direct playback (testing):
        #     # sd.play(np.array(wav_output), samplerate=tts.synthesizer.output_sample_rate)
        #     # sd.wait() 
        #     print("Synthesis complete.")

        # except Exception as e:
        #     print(f"Error with Coqui TTS: {e}")
        #     print("Please ensure you have the model downloaded and correct model name.")
        #     print("You might need to install PyTorch with CUDA support if using GPU.")


        Self-Correction: Coqui TTS setup can be tricky with dependencies and model selection. The user needs to refer to Coqui's official documentation for the most up-to-date models and usage. The above is a conceptual snippet.

    ElevenLabs (Cloud, Commercial):

        Pros: Extremely high-quality, natural voices. Very good for voice cloning. Simple API.

        Cons: Paid service (per character). Requires internet. Latency can be a factor for real-time, though usually good.

        Setup: pip install elevenlabs. Use their Python API with your API key.

Recommendation for MVP: Start with Piper due to its speed, local nature, and ease of use with pre-trained voices. This aligns with "fastest deployment" and control over resources.
5.3. File Structure Recommendation

A well-organized file structure is crucial.

lumina_project/
├── main_orchestrator.py       # Main Python script to run Lumina, manage processes
│
├── config/
│   ├── settings.yaml          # General settings, API keys (use .env for secrets!)
│   └── lumina_persona.txt     # Lumina's core system prompt for LM Studio
│
├── ai_brain/
│   └── lm_studio_interface.py # Python module to interact with LM Studio API
│
├── tts/
│   ├── tts_handler.py         # Python module to interact with chosen TTS engine
│   └── piper_models/          # Directory for Piper voice model files
│       └── en_US-ljspeech-high.onnx # Example voice
│       └── en_US-ljspeech-high.onnx.json
│
├── web_frontend/              # For Lumina's visual representation
│   ├── lumina_stage.html
│   ├── css/
│   │   └── style.css
│   ├── js/
│   │   └── animation.js
│   │   └── api_comms.js       # For WebSocket or HTTP polling
│   └── assets/
│       ├── lumina/
│       │   ├── base.png
│       │   ├── mouth_closed.png
│       │   ├── mouth_open.png
│       │   ├── eyes_open.png
│       │   ├── eyes_closed.png
│       │   └── ... (other jazz animation frames)
│       ├── dog/
│       │   ├── base.png
│       │   ├── eyes_open.png
│       │   └── ...
│       └── backgrounds/
│           └── default_bg.png
│
├── streaming/
│   ├── stream_manager.py      # Python module for YouTube API and FFmpeg control
│   └── ffmpeg_profiles.py     # (Optional) FFmpeg command templates
│
├── social_media/
│   ├── post_scheduler.py      # For triggering posts
│   ├── x_poster.py            # Script for X/Twitter
│   └── instagram_poster.py    # Script for Instagram (Post-MVP)
│
├── data_harvesting/           # Post-MVP: For news, topics
│   ├── web_scraper.py
│   └── news_api_client.py
│
├── vector_db/                 # Post-MVP: For memory
│   ├── vector_store_interface.py
│   └── embedding_utils.py
│
├── logs/                      # For application logs
│   └── lumina.log
│
├── output_clips/              # For any recorded clips/tests
│
└── .env                       # !!! IMPORTANT: For API keys, secrets. Add to .gitignore
└── requirements.txt           # Python dependencies
└── README.md                  # Project overview

5.4. Headless Live Streaming (Recap & Key Python Logic)

This combines Playwright, Xvfb (on Linux), FFmpeg, and the YouTube API.

Conceptual Python Logic (streaming/stream_manager.py):

import subprocess
import time
import os
from playwright.sync_api import sync_playwright # Or async_playwright for async app
# from googleapiclient.discovery import build # For YouTube API
# from google_auth_oauthlib.flow import InstalledAppFlow # For YouTube Auth
# from google.auth.transport.requests import Request
# import pickle

# --- YouTube API Setup (Highly Simplified - refer to official Google docs) ---
# SCOPES = ['https://www.googleapis.com/auth/youtube.force-ssl']
# API_SERVICE_NAME = 'youtube'
# API_VERSION = 'v3'
# CLIENT_SECRETS_FILE = 'config/client_secret_youtube.json' # Your OAuth client secret
# TOKEN_PICKLE_FILE = 'config/token_youtube.pickle'

# def get_youtube_service():
#     creds = None
#     if os.path.exists(TOKEN_PICKLE_FILE):
#         with open(TOKEN_PICKLE_FILE, 'rb') as token:
#             creds = pickle.load(token)
#     if not creds or not creds.valid:
#         if creds and creds.expired and creds.refresh_token:
#             creds.refresh(Request())
#         else:
#             flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)
#             # For headless, initial auth might need to be done once manually
#             # or use a service account if applicable for your use case.
#             creds = flow.run_local_server(port=0) 
#         with open(TOKEN_PICKLE_FILE, 'wb') as token:
#             pickle.dump(creds, token)
#     return build(API_SERVICE_NAME, API_VERSION, credentials=creds)

# def create_youtube_livestream(service, title, description):
#     # ... (API calls to liveStreams.insert, liveBroadcasts.insert, liveBroadcasts.bind)
#     # This returns rtmp_url and stream_key
#     print("Fetching YouTube stream details...")
#     # Placeholder - replace with actual API calls
#     time.sleep(2) # Simulate API call
#     # IMPORTANT: You MUST get these from the YouTube API for your specific stream
#     rtmp_url_base = "rtmp://a.rtmp.youtube.com/live2" 
#     stream_key = "YOUR_YOUTUBE_STREAM_KEY" # REPLACE THIS
#     print(f"RTMP URL: {rtmp_url_base}/{stream_key}")
#     return f"{rtmp_url_base}/{stream_key}"

# def start_youtube_broadcast(service, broadcast_id):
#     # ... (API call to liveBroadcasts.transition to 'live')
#     print(f"Transitioning YouTube broadcast {broadcast_id} to LIVE.")
#     pass

# --- FFmpeg and Playwright ---
XVFB_DISPLAY = ":99" # Virtual display for headless browser
BROWSER_URL = "file://" + os.path.abspath("web_frontend/lumina_stage.html")
VIDEO_RESOLUTION = "1280x720"
VIDEO_FRAMERATE = 30
# Audio source will depend on your TTS setup. This is a placeholder.
# For PulseAudio, it might be 'pulse' and a specific source.
# If piping from Python TTS, it would be '-i pipe:0' or similar.
AUDIO_INPUT_FFMPEG = "pulse" # Example: "default" for PulseAudio default source
                              # Or path to a virtual audio loopback device
                              # Or "-i pipe:0" if piping audio bytes to FFmpeg's stdin

def start_streaming_pipeline(youtube_rtmp_url):
    xvfb_process = None
    playwright_context = None
    browser = None
    ffmpeg_process = None

    try:
        # 1. Start Xvfb (Linux only)
        if os.name == 'posix': # Simplistic check for Linux-like
            print(f"Starting Xvfb on display {XVFB_DISPLAY}...")
            xvfb_command = ['Xvfb', XVFB_DISPLAY, '-screen', '0', f'{VIDEO_RESOLUTION}x24', '-ac', '-nolisten', 'tcp']
            xvfb_process = subprocess.Popen(xvfb_command)
            time.sleep(2) # Give Xvfb time to start
            os.environ['DISPLAY'] = XVFB_DISPLAY
            print("Xvfb started.")

        # 2. Start Playwright and open the page
        print("Launching headless browser with Playwright...")
        p_sync = sync_playwright().start()
        browser = p_sync.chromium.launch(headless=False if os.name == 'nt' else True) # Headless True for Linux Xvfb
        # On Windows, for testing, you might run with headless=False to see the browser
        
        # Create a new incognito browser context.
        context = browser.new_context(
            viewport={'width': int(VIDEO_RESOLUTION.split('x')[0]), 'height': int(VIDEO_RESOLUTION.split('x')[1])},
            no_viewport=False # Ensure viewport is set
        )
        page = context.new_page()
        print(f"Navigating to {BROWSER_URL}...")
        page.goto(BROWSER_URL, wait_until="load") # or "networkidle"
        print("Page loaded in headless browser.")
        # Keep page active with JavaScript if needed: page.evaluate("setInterval(() => {}, 1000)")

        # 3. Start FFmpeg
        # This command is highly dependent on your audio setup.
        # You may need to adjust -i for audio, and audio codec/bitrate.
        ffmpeg_command = [
            'ffmpeg',
            '-y',  # Overwrite output files without asking
            '-f', 'x11grab', '-video_size', VIDEO_RESOLUTION, '-framerate', str(VIDEO_FRAMERATE), '-i', os.environ.get('DISPLAY', XVFB_DISPLAY),
            # Audio input - THIS IS THE MOST SYSTEM-DEPENDENT PART
            # Example for PulseAudio default. You might need to find your specific sink/monitor.
            # Or if piping audio: '-f', 's16le', '-ar', '44100', '-ac', '1', '-i', 'pipe:0',
            '-f', AUDIO_INPUT_FFMPEG, '-i', 'default', # Placeholder, adjust for your TTS audio output
            
            '-c:v', 'libx264', '-preset', 'ultrafast', '-pix_fmt', 'yuv420p', '-s', VIDEO_RESOLUTION, '-r', str(VIDEO_FRAMERATE),
            '-g', str(VIDEO_FRAMERATE * 2), # Keyframe interval
            '-b:v', '2500k', # Video bitrate
            '-maxrate', '3000k', '-bufsize', '5000k',
            '-c:a', 'aac', '-ar', '44100', '-b:a', '128k', # Audio codec and bitrate
            '-f', 'flv',
            youtube_rtmp_url
        ]
        print(f"Starting FFmpeg with command: {' '.join(ffmpeg_command)}")
        # For debugging, capture FFmpeg's stdout and stderr
        ffmpeg_process = subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) 
        print("FFmpeg process started. Streaming...")

        # Keep the stream alive - in a real app, your main loop would be here
        # managing interactions, TTS, and signaling FFmpeg to stop when done.
        while True:
            time.sleep(1)
            # Here, you would also read ffmpeg_process.stdout and ffmpeg_process.stderr
            # to log FFmpeg output and check for errors.
            # For example:
            # for line in iter(ffmpeg_process.stderr.readline, b''):
            #    print(f"FFMPEG_ERR: {line.decode().strip()}")
            # if ffmpeg_process.poll() is not None:
            #    print("FFmpeg process terminated.")
            #    break


    except Exception as e:
        print(f"An error occurred in the streaming pipeline: {e}")
    finally:
        print("Stopping streaming pipeline...")
        if ffmpeg_process:
            print("Terminating FFmpeg...")
            if ffmpeg_process.stdin: # If you were piping audio
                 ffmpeg_process.stdin.close()
            ffmpeg_process.terminate()
            try:
                ffmpeg_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                ffmpeg_process.kill()
            print("FFmpeg terminated.")
        if browser:
            print("Closing browser...")
            browser.close()
        if playwright_context and 'p_sync' in locals(): # playwright_context is not defined here, should be p_sync
             print("Stopping Playwright...")
             p_sync.stop()
        if xvfb_process:
            print("Terminating Xvfb...")
            xvfb_process.terminate()
            xvfb_process.wait()
            print("Xvfb terminated.")
        print("Streaming pipeline stopped.")

# --- Main Orchestration (Conceptual) ---
# if __name__ == "__main__":
#     # 1. Authenticate and setup YouTube stream (get RTMP URL)
#     # youtube_service = get_youtube_service()
#     # rtmp_destination_url = create_youtube_livestream(youtube_service, "Lumina Live Test", "Testing Lumina's stream")
#     # For MVP testing without full API integration yet, get your stream key manually from YouTube Studio
#     # and construct the RTMP URL.
#     rtmp_destination_url = "rtmp://a.rtmp.youtube.com/live2/YOUR_YOUTUBE_STREAM_KEY_HERE" # REPLACE
#     if not "YOUR_YOUTUBE_STREAM_KEY_HERE" in rtmp_destination_url:
#         # 2. Start the streaming pipeline
#         # This would run in a separate thread or process in a full application
#         # to allow the main thread to handle chat, LLM, TTS.
#         start_streaming_pipeline(rtmp_destination_url)
#     else:
#         print("Please replace YOUR_YOUTUBE_STREAM_KEY_HERE in the script.")


Self-Correction on the Python snippet:

    The YouTube API part is heavily commented out and simplified because full OAuth 2.0 for a headless server is complex and requires user setup of client_secret.json. For "fastest deployment" of MVP, the user might manually get the RTMP URL/stream key from YouTube Studio first. The code provides the structure.

    Audio input for FFmpeg (AUDIO_INPUT_FFMPEG) is a major variable. It needs to be configured based on how TTS audio is made available to FFmpeg on the specific OS. PulseAudio loopbacks or ALSA loopbacks on Linux are common for capturing application audio.

    Error handling and process management in a real "always running" system would need to be much more robust (e.g., using asyncio for better concurrency, watchdog timers, proper logging to files).

    The playwright_context variable was not correctly referenced in the finally block; it should be p_sync that's stopped. Corrected conceptually.

    The main loop in start_streaming_pipeline is just a while True: time.sleep(1). In reality, this function would likely be run in a thread, and the main application loop would handle interactions and decide when to stop the stream. The FFmpeg error log reading is also essential for debugging.

Section 6: Considerations & Future Steps

    Hardware Progression: Your plan (3090Ti -> Dual 3090Ti -> 48GB VRAM -> Cloud) is solid. Monitor VRAM usage closely, especially when running the LLM, headless browser, and potentially local TTS simultaneously.

    Ethical AI: As Lumina and other AI agents evolve, continuously consider the ethical implications of their interactions, potential biases in their training data or LLM responses, and their influence. Transparency with the audience about Lumina being an AI is important.

    Community Building: Actively engage with your audience. Their feedback will be invaluable for shaping Lumina's development and the world she inhabits.

    Monetization (Future): While not the current focus, the interactive nature and unique content could open avenues like YouTube Super Chat, channel memberships, sponsored segments (handled ethically by Lumina), or even unique NFT-gated experiences/interactions post-MVP.

    Scalability of "Infinite Canvas": While conceptually infinite, rendering an extremely large and complex HTML DOM can hit browser performance limits. Strategies like virtual scrolling (only rendering visible parts), LOD (Level of Detail) for distant elements, and optimizing JS will be needed as the world grows.

Section 7: Questions for You

To refine this plan further, especially the "Fastest Deployment" aspects:

    Lumina & Dog - Visual Style Details: While you have the character set, are there any specific key poses or simple "jazz" animations you absolutely want for the MVP? (e.g., a signature thinking pose for Lumina, a specific happy animation for the dog).

    Initial Social Media Platform: For the MVP's social media posting, which single platform (e.g., X, Instagram, TikTok) is your top priority to start with?
    Comfort with OS-Level Configuration: Setting up audio loopbacks for FFmpeg to capture TTS audio can involve some OS-specific configuration (e.g., PulseAudio on Linux). How comfortable are you with this, or would you prefer a TTS that can more easily pipe audio directly to FFmpeg if available?

    Existing Code/Assets: You mentioned the character and set are built. Are these in layered PNGs ready for web use, or do they need conversion/exporting? Any existing Python scripts you plan to integrate?

This document provides a comprehensive blueprint. The journey to bring Lumina and her world to life will be iterative and filled with learning. Focus on the MVP phases, get Lumina streaming and interacting, and then progressively build out the incredible, dynamic world you envision! Your emphasis on interactivity is spot on for creating something truly special.